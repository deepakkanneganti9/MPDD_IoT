Machine Learning as a Service (MLaaS) is a powerful cloud ser-
vice that enables data-driven intelligent applications in Internet
of Things (IoT) environments. Many IoT applications across smart
healthcare, transportation, homes, cities, and industry rely on MLaaS
due to its cost-effectiveness. The dynamic nature of IoT causes fre-
quent changes in data distribution, and characteristics affect the
stability of MLaaS performance. Moreover, MLaaS providers peri-
odically update can further impact the predictive accuracy of the
service and cause performance drift. Unlike traditional ML, devel-
opers can retrain with access to data and parameters. MLaaS clients
are black-box users without this control, making performance drift
detection particularly challenging. To address this, we propose a
novel MLaaS performance drift detection framework for IoT envi-
ronments. The framework first employs an MLaaS extraction model
that learns the provider‚Äôs behaviour from input‚Äìoutput pairs, re-
vealing prediction-influenced features of MLaaS. Building on this,
we propose a novel MLaaS Performance Drift Detection (MPDD)
model that jointly captures changes in input data and MLaaS be-
haviour. We introduce the Adaptive-Temporal Performance Drift
Detection mechanism (APDDM), which is a variable interval mech-
anism to balance oversensitivity and undersensitivity. Experimental
results on real-world datasets show that the proposed MPDD model
achieves up to 15 to 20 % higher accuracy, while the APDDM mech-
anism achieves 65.19 % fewer false negatives compared with the
fixed-interval approach

## üìò Overview

The experiments collectively evaluate:
- **Performance Drift in MLaaS systems** where internal model access is restricted.  
- **Adaptive monitoring mechanisms** that automatically tune drift detection sensitivity.
- **Impact of data drift** (e.g., user behavior, environmental changes, or market variations) on MLaaS prediction accuracy.

The framework leverages **synthetic drift injection**, **MLaaS extraction modeling**, and **ground truth evaluation** to systematically assess detection accuracy, delay, and false alarms.

---

## ‚öôÔ∏è Common Workflow

All notebooks follow a consistent experimental workflow:

1. **Load and preprocess dataset**  
   Each dataset (HAR, Electricity, Weather, or Benchmark) is prepared and normalized.

2. **Inject synthetic drift**  
   Drift is introduced artificially to simulate real-world distribution shifts such as:
   - Sudden, gradual, or incremental changes  
   - Seasonal or behavioral variations  

3. **Train MLaaS Extraction Model**  
   - A lightweight surrogate model learns the mapping between inputs and MLaaS predictions.  
   - Used to observe **black-box behaviour** and detect hidden performance degradation.

4. **Evaluate Performance Drift**  
   - Compare MLaaS predictions with true labels.  
   - Compute metrics: accuracy degradation, detection delay, false positives, and pseudo drift rate.

5. **Visualize Results**  
   - Plots showing drift points, detection timelines, and adaptive interval variations.  
   - Summary tables with comparative metrics.

---

## üß™ Experiment Summaries

### **Experiment 1: PMDD Framework Evaluation**

This experiment demonstrates the **Performance Drift Detection (PMDD)** framework using multiple real-world datasets to assess its robustness under different domain conditions.

#### üìÇ File 1: `Experiment_1_PMDD_Framework_HAR_Dataset_Final_v2.ipynb`
- **Dataset:** Human Activity Recognition (HAR)  
- **Goal:** Detect accuracy degradation in MLaaS predictions caused by user or sensor drift.  
- **Unique Aspect:** Focuses on human-centric data and sensor variability.  
- **Outputs:** Drift detection plots and accuracy degradation summaries.

#### üìÇ File 2: `Experiment_1_PMDD_Framework_Electricity_Dataset.ipynb`
- **Dataset:** Electricity Market Data  
- **Goal:** Detect real vs. pseudo drift events due to changes in electricity demand and pricing.  
- **Unique Aspect:** Evaluates MLaaS adaptability in fluctuating market conditions.  
- **Outputs:** Visualizations of drift intervals, adaptive vs. fixed detection comparisons, and event logs.

#### üìÇ File 3: `Experiment_1_PMDD_Framework_Weather_Dataset.ipynb`
- **Dataset:** Weather/Climate Data  
- **Goal:** Monitor MLaaS stability under gradual and seasonal drifts.  
- **Unique Aspect:** Highlights environmental dynamics affecting MLaaS prediction stability.  
- **Outputs:** Drift detection plots and comparison of adaptive vs. static monitoring intervals.

---

### **Experiment 2: Adaptive Temporal Mechanism Evaluation**

#### üìÇ File: `Experiment_2_Adaptive_Temporal_PDMM_Benchmark_dataset_Final.ipynb`

This notebook extends the PMDD framework with an **Adaptive Temporal Performance Drift Detection Mechanism (APDDM)**.

- **Objective:** Automatically adjust monitoring intervals based on drift trends.  
- **Mechanism:**
  - **Shortens** intervals when real drift persists (for rapid response).  
  - **Extends** intervals when pseudo drift dominates (to reduce false alarms).  
- **Outputs:**
  - Comparative plots of adaptive vs. fixed detection behavior.
  - Metrics: Miss Detection (MD), False Negatives (FN), False Positives (FP), and Detection Delay.
- **Unique Aspect:** Demonstrates **timeliness and responsiveness** of adaptive drift detection in dynamic MLaaS systems.

---

## üìä Outputs Across Experiments

- Drift detection plots over time.  
- Adaptive vs. fixed window performance comparisons.  
- Summary metrics including:
  - **Accuracy degradation**
  - **Detection delay**
  - **False positive/negative rates**
  - **Window sensitivity evolution**

---

## üß© Dependencies

- **Python:** 3.9 or later  
- **Libraries:**  
  `numpy`, `pandas`, `matplotlib`, `scikit-learn`, `seaborn`, `tensorflow` or `torch`

---

## üìÅ Repository Structure 
‚îú‚îÄ‚îÄ Experiment_1_PMDD_Framework_HAR_Dataset_Final_v2.ipynb
‚îú‚îÄ‚îÄ Experiment_1_PMDD_Framework_Electricity_Dataset_Final.ipynb
‚îú‚îÄ‚îÄ Experiment_1_PMDD_Framework_Weather_Dataset.ipynb
‚îî‚îÄ‚îÄ Experiment_2_Adaptive_Temporal_PDMM_Benchmark_dataset_Final.ipynb
