Machine Learning as a Service (MLaaS) is a powerful cloud paradigm enabling data-driven intelligent applications in Internet of Things (IoT) environments, widely adopted across healthcare, smart homes, and industry due to its cost-effectiveness. However, the dynamic nature of IoT frequently alters data distributions, affecting MLaaS stability, while periodic MLaaS updates further introduce performance drift. Unlike traditional ML systems, MLaaS clients operate as black-box users without access to internal data or parameters, making drift detection particularly challenging. To address this, we propose a novel MLaaS Performance Drift Detection framework for IoT environments. The framework first employs an MLaaS extraction model that learns service behavior from input‚Äìoutput pairs and identifies prediction-influenced features. Building on this, the proposed MLaaS Performance Drift Detection (MPDD) model jointly captures variations in input data and MLaaS behavior. We further design an Adaptive-Temporal Performance Drift Detection Mechanism (APDDM) that dynamically adjusts monitoring frequency based on behavioral and data variations, enabling timely drift detection for effective service management. Extensive experiments on real-world datasets demonstrate that MPDD achieves up to 20‚Äì25\% accuracy improvement over baseline drift detection methods, while the adaptive APDDM provides an additional average accuracy gain of approximately 15\% and reduces the miss detection rate by around 10\% compared to fixed-interval monitoring. 

## üìò Overview

The experiments collectively evaluate:
- **Performance Drift in MLaaS systems** where internal model access is restricted.  
- **Adaptive monitoring mechanisms** that automatically tune drift detection sensitivity.
- **Impact of data drift** (e.g., user behavior, environmental changes, or market variations) on MLaaS prediction accuracy.

The framework leverages **synthetic drift injection**, **MLaaS extraction modeling**, and **ground truth evaluation** to systematically assess detection accuracy, delay, and false alarms.

---

## ‚öôÔ∏è Common Workflow

All notebooks follow a consistent experimental workflow:

1. **Load and preprocess dataset**  
   Each dataset (HAR, Electricity, Weather, or Benchmark) is prepared and normalized.

2. **Inject synthetic drift**  
   Drift is introduced artificially to simulate real-world distribution shifts such as:
   - Sudden, gradual, or incremental changes  
   - Seasonal or behavioral variations  

3. **Train MLaaS Extraction Model**  
   - A lightweight surrogate model learns the mapping between inputs and MLaaS predictions.  
   - Used to observe **black-box behaviour** and detect hidden performance degradation.

4. **Evaluate Performance Drift**  
   - Compare MLaaS predictions with true labels.  
   - Compute metrics: accuracy degradation, detection delay, false positives, and pseudo drift rate.

5. **Visualize Results**  
   - Plots showing drift points, detection timelines, and adaptive interval variations.  
   - Summary tables with comparative metrics.

---

## üß™ Experiment Summaries

### **Experiment 1: PMDD Framework Evaluation**

This experiment demonstrates the **Performance Drift Detection (PMDD)** framework using multiple real-world datasets to assess its robustness under different domain conditions.

#### üìÇ File 1: `Experiment_1_PMDD_Framework_HAR_Dataset_Final_v2.ipynb`
- **Dataset:** Human Activity Recognition (HAR)  
- **Goal:** Detect accuracy degradation in MLaaS predictions caused by user or sensor drift.  
- **Unique Aspect:** Focuses on human-centric data and sensor variability.  
- **Outputs:** Drift detection plots and accuracy degradation summaries.

#### üìÇ File 2: `Experiment_1_PMDD_Framework_Electricity_Dataset.ipynb`
- **Dataset:** Electricity Market Data  
- **Goal:** Detect real vs. pseudo drift events due to changes in electricity demand and pricing.  
- **Unique Aspect:** Evaluates MLaaS adaptability in fluctuating market conditions.  
- **Outputs:** Visualizations of drift intervals, adaptive vs. fixed detection comparisons, and event logs.

#### üìÇ File 3: `Experiment_1_PMDD_Framework_Weather_Dataset.ipynb`
- **Dataset:** Weather/Climate Data  
- **Goal:** Monitor MLaaS stability under gradual and seasonal drifts.  
- **Unique Aspect:** Highlights environmental dynamics affecting MLaaS prediction stability.  
- **Outputs:** Drift detection plots and comparison of adaptive vs. static monitoring intervals.

---

### **Experiment 2: Adaptive Temporal Mechanism Evaluation**

#### üìÇ File: `Experiment_2_Adaptive_Temporal_PDMM_Benchmark_dataset_Final.ipynb`

This notebook extends the PMDD framework with an **Adaptive Temporal Performance Drift Detection Mechanism (APDDM)**.

- **Objective:** Automatically adjust monitoring intervals based on drift trends.  
- **Mechanism:**
  - **Shortens** intervals when real drift persists (for rapid response).  
  - **Extends** intervals when pseudo drift dominates (to reduce false alarms).  
- **Outputs:**
  - Comparative plots of adaptive vs. fixed detection behavior.
  - Metrics: Miss Detection (MD), False Negatives (FN), False Positives (FP), and Detection Delay.
- **Unique Aspect:** Demonstrates **timeliness and responsiveness** of adaptive drift detection in dynamic MLaaS systems.

---

## üìä Outputs Across Experiments

- Drift detection plots over time.  
- Adaptive vs. fixed window performance comparisons.  
- Summary metrics including:
  - **Accuracy degradation**
  - **Detection delay**
  - **False positive/negative rates**
  - **Window sensitivity evolution**

---

## üß© Dependencies

- **Python:** 3.9 or later  
- **Libraries:**  
  `numpy`, `pandas`, `matplotlib`, `scikit-learn`, `seaborn`, `tensorflow` or `torch`

---

## üìÅ Repository Structure 
‚îú‚îÄ‚îÄ Experiment_1_PMDD_Framework_HAR_Dataset_Final_v2.ipynb
‚îú‚îÄ‚îÄ Experiment_1_PMDD_Framework_Electricity_Dataset_Final.ipynb
‚îú‚îÄ‚îÄ Experiment_1_PMDD_Framework_Weather_Dataset.ipynb
‚îî‚îÄ‚îÄ Experiment_2_Adaptive_Temporal_PDMM_Benchmark_dataset_Final.ipynb
